这个项目是kaggle最简单的竞赛之一，现在才开始机器学习竞赛的探索似乎有点晚，但是也不算太晚。我完成了titanic的项目之后觉得那个项目的编排和文件略显粗糙，决定在进行这次的竞赛之余好好地git和记录文件。
这个项目比起titanic来说是一个回归项目，最后输出一个预测数值而不是二分类。而且特征极其多（似乎有80个？），于是特征工程的重要性又被提高到了一个新的高度，但也有好处是上限很高。
首先我们使用了一个最简单的pipeline先得出一个数值，先把模型跑通，这是一个很重要的习惯。具体的细节在CHANGELOG.md里，这里就不多赘述了。特征工程中最先学习的是补值，用中位数和众数补值是最粗暴有效的。这里首先将空值补为了none值，比如是否有泳池等，因为这也是一个特征。在这里用众数补值显然是愚蠢的。。。
补值后的随机森林结果从0.14561降低到了0.14544，确实有正面的提升但是显然远远不够，可能是随机森林没有挑选到我们处理过的特征。随后换用LightGBM得到0.13702，XGB得到0.13893，融合两者得到0.13590。强势的模型确实显著地有效，融合也是好策略。
最终提交的时候出现了前面没有出现过的类，最终还是在encoder的时候把它们标记出来才得到了结果。