{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1.数据加载与切分","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# --- 我们先重新加载和切分数据，确保所有步骤都在一个代码块里 ---\n# ## 1. 数据加载与切分\n\n# 加载数据\nfile_path = '/kaggle/input/house-prices-advanced-regression-techniques/train.csv'\ntrain_full = pd.read_csv(file_path)\n\n# 分离特征 (X) 和目标 (y)\nX_full = train_full.drop('SalePrice', axis=1)\ny_full = train_full['SalePrice']\n\n# --- **就放在这里！** ---\n# 在切分之前，就从总的特征集里把Id去掉\nX_full_no_id = X_full.drop('Id', axis=1)\n# -------------------------\n\n# 然后，用不含Id的特征集去进行切分\nX_train, X_val, y_train, y_val = train_test_split(X_full_no_id, y_full, test_size=0.2, random_state=42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:21:08.400284Z","iopub.execute_input":"2025-07-20T10:21:08.400493Z","iopub.status.idle":"2025-07-20T10:21:11.649097Z","shell.execute_reply.started":"2025-07-20T10:21:08.400466Z","shell.execute_reply":"2025-07-20T10:21:11.648198Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"2.对数变换","metadata":{}},{"cell_type":"code","source":"\n# 对目标变量y进行对数变换\ny_train_log = np.log1p(y_train)\ny_val_log = np.log1p(y_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:21:14.471909Z","iopub.execute_input":"2025-07-20T10:21:14.472591Z","iopub.status.idle":"2025-07-20T10:21:14.480147Z","shell.execute_reply.started":"2025-07-20T10:21:14.472555Z","shell.execute_reply":"2025-07-20T10:21:14.479112Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"3.特征工程","metadata":{}},{"cell_type":"markdown","source":"3.1 初级特征工程","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# 这是一个独立的函数，我们可以反复调用它\ndef feature_engineer_advanced(df):\n    \"\"\"\n    对数据集进行高级特征工程，包括智能填充和特征创造。\n    \"\"\"\n    # 为了不修改原始数据，先创建副本\n    df_fe = df.copy()\n\n    # --- 1. 智能填充：NaN 代表 \"没有\" ---\n    \n    # 填充类别型特征\n    for col in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n                'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n                'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n                'MasVnrType']:\n        df_fe[col] = df_fe[col].fillna('None')\n\n    # 填充数值型特征\n    for col in ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2',\n                'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n                'MasVnrArea']:\n        df_fe[col] = df_fe[col].fillna(0)\n\n    # --- 2. 智能填充：用分组中位数填充 \"真缺失\" ---\n\n    # LotFrontage 很可能和街区(Neighborhood)有关\n    # 我们用每个街区LotFrontage的中位数来填充该街区的缺失值\n    df_fe['LotFrontage'] = df_fe.groupby('Neighborhood')['LotFrontage'].transform(\n        lambda x: x.fillna(x.median())\n    )\n\n    # 对于其他一些\"真缺失\"，我们暂时还用众数填充，作为保底策略\n    for col in ['MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType']:\n        df_fe[col] = df_fe[col].fillna(df_fe[col].mode()[0])\n\n\n    # --- 3. 创造新特征 (和我们上次做的一样) ---\n\n    df_fe['TotalSF'] = df_fe['TotalBsmtSF'] + df_fe['1stFlrSF'] + df_fe['2ndFlrSF']\n    df_fe['HouseAge'] = df_fe['YrSold'] - df_fe['YearBuilt']\n    df_fe['RemodAge'] = df_fe['YrSold'] - df_fe['YearRemodAdd']\n    df_fe['TotalBath'] = df_fe['BsmtFullBath'] + (0.5 * df_fe['BsmtHalfBath']) + \\\n                         df_fe['FullBath'] + (0.5 * df_fe['HalfBath'])\n    \n    # --- 4. (可选) 删除一些不再需要的原始列 ---\n    # 比如我们已经有了HouseAge，YearBuilt可能就不那么重要了\n    # df_fe = df_fe.drop(['YearBuilt', 'YearRemodAdd'], axis=1)\n\n    print(\"✅ 高级特征工程函数已定义。\")\n    return df_fe\n\n# --- 现在，我们用这个新函数来处理我们的训练集和验证集 ---\n# 假设 X_train 和 X_val 已经存在\nX_train_advanced = feature_engineer_advanced(X_train)\nX_val_advanced = feature_engineer_advanced(X_val)\n\nprint(\"\\n查看一下处理后的数据，确认缺失值是否已被处理：\")\n# 检查处理后的训练集是否还有缺失值\nmissing_after = X_train_advanced.isnull().sum().sum()\nprint(f\"处理后，训练集剩余缺失值总数: {missing_after}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:21:17.011910Z","iopub.execute_input":"2025-07-20T10:21:17.012210Z","iopub.status.idle":"2025-07-20T10:21:17.072978Z","shell.execute_reply.started":"2025-07-20T10:21:17.012190Z","shell.execute_reply":"2025-07-20T10:21:17.072206Z"}},"outputs":[{"name":"stdout","text":"✅ 高级特征工程函数已定义。\n✅ 高级特征工程函数已定义。\n\n查看一下处理后的数据，确认缺失值是否已被处理：\n处理后，训练集剩余缺失值总数: 0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import skew\n\ndef feature_engineer_final_robust(df, train_df_for_skew_calc=None, skew_list_to_apply=None):\n    \"\"\"\n    对数据集进行最终的、更健壮的特征工程。\n    遵循“先清洗、后计算”的原则。\n    \"\"\"\n    df_fe = df.copy()\n\n    # --- 阶段一：彻底清洗缺失值 ---\n\n    # 1. 填充 \"NaN\" 代表 \"没有\" 的类别特征\n    for col in ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature', 'MasVnrType']:\n        df_fe[col] = df_fe[col].fillna('None')\n\n    # 2. 填充 \"NaN\" 代表 \"0\" 的数值特征\n    for col in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea']:\n        df_fe[col] = df_fe[col].fillna(0)\n\n    # 3. 填充 \"真缺失\" 的类别特征 (用众数)\n    for col in ['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'Electrical', 'KitchenQual', 'Functional', 'SaleType']:\n        if col in df_fe.columns:\n            df_fe[col] = df_fe[col].fillna(df_fe[col].mode()[0])\n\n    # 4. 填充 \"真缺失\" 的数值特征 (用中位数)\n    # 这是一个“保底”步骤，确保所有我们没明确处理的数值缺失值都被填充\n    numerical_cols_with_na = df_fe.select_dtypes(include=np.number).columns[df_fe.select_dtypes(include=np.number).isnull().any()]\n    for col in numerical_cols_with_na:\n        df_fe[col] = df_fe[col].fillna(df_fe[col].median())\n\n\n    # --- 阶段二：在干净的数据上进行特征创造与变换 ---\n\n    # 5. 基础特征创造\n    df_fe['TotalSF'] = df_fe['TotalBsmtSF'] + df_fe['1stFlrSF'] + df_fe['2ndFlrSF']\n    df_fe['HouseAge'] = df_fe['YrSold'] - df_fe['YearBuilt']\n    df_fe['RemodAge'] = df_fe['YrSold'] - df_fe['YearRemodAdd']\n    df_fe['TotalBath'] = df_fe['BsmtFullBath'] + (0.5 * df_fe['BsmtHalfBath']) + df_fe['FullBath'] + (0.5 * df_fe['HalfBath'])\n    \n    # 6. 处理年龄为负数的边界情况\n    df_fe['HouseAge'] = df_fe['HouseAge'].clip(0)\n    df_fe['RemodAge'] = df_fe['RemodAge'].clip(0)\n\n    # 7. 创建交互特征\n    df_fe['Qual_x_TotalSF'] = df_fe['OverallQual'] * df_fe['TotalSF']\n    df_fe['Qual_x_HouseAge'] = df_fe['OverallQual'] * df_fe['HouseAge']\n    \n    # 8. 处理数值特征倾斜度 (正态化)\n    if train_df_for_skew_calc is not None:\n        numerical_feats = train_df_for_skew_calc.select_dtypes(exclude=\"object\").columns\n        skewed_feats = train_df_for_skew_calc[numerical_feats].apply(lambda x: skew(x.dropna()))\n        skew_list_to_apply = skewed_feats[skewed_feats > 0.75].index\n        print(f\"从训练集中发现 {len(skew_list_to_apply)} 个高度倾斜的特征将被变换。\")\n    \n    if skew_list_to_apply is not None:\n        for feat in skew_list_to_apply:\n            if feat in df_fe.columns:\n                df_fe[feat] = np.log1p(df_fe[feat])\n    \n    return df_fe, skew_list_to_apply\n\n# --- 现在，我们用这个最终修正版的函数来处理数据 ---\n# 假设 X_train 和 X_val 已经存在\n\n# 1. 在训练集上应用，并获取倾斜特征列表\nX_train_final, skew_list = feature_engineer_final_robust(X_train, train_df_for_skew_calc=X_train)\n\n# 2. 在验证集上应用，并传入从训练集学到的倾斜列表\nX_val_final, _ = feature_engineer_final_robust(X_val, skew_list_to_apply=skew_list)\n\n\nprint(\"\\n✅ 终极健壮版特征工程执行完毕！\")\nmissing_after = X_train_final.isnull().sum().sum()\nprint(f\"处理后，训练集剩余缺失值总数: {missing_after}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T11:04:19.366803Z","iopub.execute_input":"2025-07-20T11:04:19.367159Z","iopub.status.idle":"2025-07-20T11:04:19.589065Z","shell.execute_reply.started":"2025-07-20T11:04:19.367135Z","shell.execute_reply":"2025-07-20T11:04:19.588367Z"}},"outputs":[{"name":"stdout","text":"从训练集中发现 21 个高度倾斜的特征将被变换。\n\n✅ 终极健壮版特征工程执行完毕！\n处理后，训练集剩余缺失值总数: 0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"3.2 暂时最终版特征工程","metadata":{}},{"cell_type":"markdown","source":"3.2.1 清洗","metadata":{}},{"cell_type":"code","source":"# ## 1. 数据加载、清洗与切分\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# 加载数据\nfile_path = '/kaggle/input/house-prices-advanced-regression-techniques/train.csv'\ntrain_full = pd.read_csv(file_path)\n\n# 【新增】移除著名的异常值\noutlier_indices = train_full[(train_full['GrLivArea'] > 4000) & (train_full['SalePrice'] < 300000)].index\ntrain_full = train_full.drop(outlier_indices)\nprint(f\"移除了 {len(outlier_indices)} 个GrLivArea异常值。\")\n\n# 分离特征和目标\nX_full = train_full.drop('SalePrice', axis=1)\ny_full = train_full['SalePrice']\n\n# 舍去Id列\nX_full = X_full.drop('Id', axis=1)\n\n# 切分数据\nX_train, X_val, y_train, y_val = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n\n# 对目标变量y进行对数变换\ny_train_log = np.log1p(y_train)\ny_val_log = np.log1p(y_val)\n\nprint(\"数据加载、异常值移除、切分完成！\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T11:27:26.469079Z","iopub.execute_input":"2025-07-20T11:27:26.469384Z","iopub.status.idle":"2025-07-20T11:27:26.504826Z","shell.execute_reply.started":"2025-07-20T11:27:26.469365Z","shell.execute_reply":"2025-07-20T11:27:26.503941Z"}},"outputs":[{"name":"stdout","text":"移除了 2 个GrLivArea异常值。\n数据加载、异常值移除、切分完成！\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"3.2.2 特征工程函数","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom scipy.stats import skew\n\ndef feature_engineer_ultimate_final(df, skew_list_to_apply=None):\n    \"\"\"\n    终极修正版特征工程函数，修复了类型转换和倾斜度计算的顺序问题。\n    \"\"\"\n    df_fe = df.copy()\n\n    # --- 1. 类型转换 (必须在最前面) ---\n    df_fe['MSSubClass'] = df_fe['MSSubClass'].astype(str)\n\n    # --- 2. 有序特征的数值映射 ---\n    quality_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\n    ordered_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\n    for col in ordered_cols:\n        if col in df_fe.columns:\n            df_fe[col] = df_fe[col].map(quality_map)\n\n    # --- 3. 智能填充 ---\n    # (这部分代码保持不变)\n    for col in ['Alley', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Fence', 'GarageType', 'GarageFinish', 'MasVnrType', 'MiscFeature']:\n         df_fe[col] = df_fe[col].fillna('None')\n    for col in ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'] + ordered_cols:\n        df_fe[col] = df_fe[col].fillna(0)\n    df_fe['LotFrontage'] = df_fe.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n    for col in ['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'Electrical', 'Functional', 'SaleType']:\n        if col in df_fe.columns: df_fe[col] = df_fe[col].fillna(df_fe[col].mode()[0])\n    \n    # 对任何可能遗漏的数值列进行中位数填充\n    numerical_cols_with_na = df_fe.select_dtypes(include=np.number).columns[df_fe.select_dtypes(include=np.number).isnull().any()]\n    for col in numerical_cols_with_na:\n        df_fe[col] = df_fe[col].fillna(df_fe[col].median())\n\n\n    # --- 4. 基础特征创造 ---\n    # (这部分代码保持不变)\n    df_fe['TotalSF'] = df_fe['TotalBsmtSF'] + df_fe['1stFlrSF'] + df_fe['2ndFlrSF']\n    df_fe['HouseAge'] = df_fe['YrSold'] - df_fe['YearBuilt']\n    df_fe['RemodAge'] = df_fe['YrSold'] - df_fe['YearRemodAdd']\n    df_fe['TotalBath'] = df_fe['BsmtFullBath'] + (0.5 * df_fe['BsmtHalfBath']) + df_fe['FullBath'] + (0.5 * df_fe['HalfBath'])\n\n    # --- 5. 处理年龄为负数的边界情况 ---\n    # (这部分代码保持不变)\n    df_fe['HouseAge'] = df_fe['HouseAge'].clip(0)\n    df_fe['RemodAge'] = df_fe['RemodAge'].clip(0)\n\n    # --- 6. 创建交互特征 ---\n    # (这部分代码保持不变)\n    df_fe['Qual_x_TotalSF'] = df_fe['OverallQual'] * df_fe['TotalSF']\n    df_fe['Qual_x_HouseAge'] = df_fe['OverallQual'] * df_fe['HouseAge']\n    \n    # --- 7. 处理数值特征倾斜度 (正态化) ---\n    # 【修正】: 在转换后的数据上识别数值列\n    if skew_list_to_apply is None: # 如果是训练集...\n        print(\"正在计算训练集的数值特征倾斜度...\")\n        # 核心修正：在已经转换了MSSubClass的df_fe上识别数值列\n        numerical_feats = df_fe.select_dtypes(exclude=[\"object\", \"category\"]).columns\n        skewed_feats = df_fe[numerical_feats].apply(lambda x: skew(x.dropna()))\n        skew_list_to_apply = skewed_feats[skewed_feats > 0.5].index\n        print(f\"发现 {len(skew_list_to_apply)} 个高度倾斜的特征。\")\n    \n    # 对所有倾斜的特征应用log1p变换\n    for feat in skew_list_to_apply:\n        df_fe[feat] = np.log1p(df_fe[feat])\n    \n    return df_fe, skew_list_to_apply\n\n# --- 调用最终版函数 (调用方式不变) ---\nX_train_pro, skew_list_pro = feature_engineer_ultimate_final(X_train)\nX_val_pro, _ = feature_engineer_ultimate_final(X_val, skew_list_to_apply=skew_list_pro)\n\nprint(\"\\n✅ 终极Pro版特征工程执行完毕！\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T11:28:44.562372Z","iopub.execute_input":"2025-07-20T11:28:44.562704Z","iopub.status.idle":"2025-07-20T11:28:44.684295Z","shell.execute_reply.started":"2025-07-20T11:28:44.562680Z","shell.execute_reply":"2025-07-20T11:28:44.683580Z"}},"outputs":[{"name":"stdout","text":"正在计算训练集的数值特征倾斜度...\n发现 31 个高度倾斜的特征。\n\n✅ 终极Pro版特征工程执行完毕！\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"4.预处理pipeline","metadata":{}},{"cell_type":"code","source":"# --- ## 4. 数据预处理流水线 (Preprocessing Pipeline) ---\n\n# 首先，我们从经过特征工程的数据中舍去Id列\n# （虽然我们之前的函数没用到Id，但在这里再确认一遍是好习惯）\nif 'Id' in X_train_advanced.columns:\n    X_train_final = X_train_advanced.drop('Id', axis=1)\n    X_val_final = X_val_advanced.drop('Id', axis=1)\nelse:\n    X_train_final = X_train_advanced\n    X_val_final = X_val_advanced\n\n# 1. 重新定义数值列和类别列的列表\n# 因为我们增加了新特征（如TotalSF），所以需要更新列表\ncategorical_cols = [cname for cname in X_train_final.columns if X_train_final[cname].dtype == \"object\"]\nnumerical_cols = [cname for cname in X_train_final.columns if X_train_final[cname].dtype in ['int64', 'float64']]\n\n# 2. 创建预处理管道 (和之前一样，但现在作用于新数据)\n# 对于数值型特征，我们只保留一个中位数填充器，以防万一有未处理的缺失值\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# 对于类别型特征，我们只需要进行独热编码\n# 因为我们的特征工程函数已经填充了所有类别缺失值\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# 3. 组合成一个总的预处理器\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# 4. 用预处理器来处理经过高级特征工程的数据\nX_train_processed = preprocessor.fit_transform(X_train_final)\nX_val_processed = preprocessor.transform(X_val_final)\n\n\n# 5. 查看最终处理后的数据形状\nprint(\"高级特征工程后，训练集形状:\", X_train_final.shape)\nprint(\"最终处理后，训练集形状:\", X_train_processed.shape)\nprint(\"\\n高级特征工程后，验证集形状:\", X_val_final.shape)\nprint(\"最终处理后，验证集形状:\", X_val_processed.shape)\n\nprint(\"\\n✅ 所有数据准备工作已完成！现在可以进入模型训练了。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T10:21:20.387757Z","iopub.execute_input":"2025-07-20T10:21:20.388087Z","iopub.status.idle":"2025-07-20T10:21:20.466840Z","shell.execute_reply.started":"2025-07-20T10:21:20.388060Z","shell.execute_reply":"2025-07-20T10:21:20.465938Z"}},"outputs":[{"name":"stdout","text":"高级特征工程后，训练集形状: (1168, 83)\n最终处理后，训练集形状: (1168, 304)\n\n高级特征工程后，验证集形状: (292, 83)\n最终处理后，验证集形状: (292, 304)\n\n✅ 所有数据准备工作已完成！现在可以进入模型训练了。\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"4# 预处理最终版","metadata":{}},{"cell_type":"code","source":"# ## 4. 数据预处理流水线 (最终健壮版)\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# 我们的输入是 X_train_pro 和 X_val_pro\n\n# 1. 重新定义数值列和类别列的列表 (不变)\ncategorical_cols = [cname for cname in X_train_pro.columns if X_train_pro[cname].dtype == \"object\"]\nnumerical_cols = [cname for cname in X_train_pro.columns if X_train_pro[cname].dtype in ['int64', 'float64']]\n\n# --- 【核心修正】 ---\n# 2. 从训练集中为每个类别特征提取“标准答案”（所有可能的类别）\ncategories_list = [X_train_pro[col].unique().tolist() for col in categorical_cols]\n\n# 3. 创建预处理管道\n# 数值管道不变\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# 类别管道现在使用我们提供的“标准答案”\ncategorical_transformer = Pipeline(steps=[\n    # 明确告知编码器所有可能的类别\n    ('onehot', OneHotEncoder(handle_unknown='ignore', categories=categories_list))\n])\n# --------------------\n\n# 4. 组合成一个总的预处理器\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ],\n    remainder='passthrough'\n)\n\n# 5. 用预处理器来处理我们的“终极”特征数据\nX_train_processed_final = preprocessor.fit_transform(X_train_pro)\nX_val_processed_final = preprocessor.transform(X_val_pro)\n\n# 6. 检查最终处理后的数据形状，确保它们一致\nprint(\"终极特征工程后，训练集形状:\", X_train_pro.shape)\nprint(\"最终预处理后，训练集形状:\", X_train_processed_final.shape)\nprint(\"\\n终极特征工程后，验证集形状:\", X_val_pro.shape)\nprint(\"最终预处理后，验证集形状:\", X_val_processed_final.shape)\n\n# 检查列数是否一致\nif X_train_processed_final.shape[1] == X_val_processed_final.shape[1]:\n    print(\"\\n✅ 特征数量一致！问题已解决。\")\nelse:\n    print(\"\\n‼️ 警告：特征数量仍然不一致，请检查代码。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:26:08.784131Z","iopub.execute_input":"2025-07-20T13:26:08.784461Z","iopub.status.idle":"2025-07-20T13:26:08.863404Z","shell.execute_reply.started":"2025-07-20T13:26:08.784438Z","shell.execute_reply":"2025-07-20T13:26:08.862602Z"}},"outputs":[{"name":"stdout","text":"终极特征工程后，训练集形状: (1166, 85)\n最终预处理后，训练集形状: (1166, 278)\n\n终极特征工程后，验证集形状: (292, 85)\n最终预处理后，验证集形状: (292, 278)\n\n✅ 特征数量一致！问题已解决。\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"5.1.a 使用random forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_log_error\nimport numpy as np\n\n# --- 再次训练模型，但这次用的是经过高级特征工程的数据 ---\n\n# 我们可以叫它 model_v2，以区别于之前的基准模型\nmodel_v2 = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\nprint(\"正在使用加强版特征数据训练新模型...\")\n# ‼️注意：我们用的是 X_train_processed 和 y_train_log\nmodel_v2.fit(X_train_processed, y_train_log)\nprint(\"✅ 新模型训练完成！\")\n\n# 在验证集上进行预测\nval_preds_log_v2 = model_v2.predict(X_val_processed)\n\n# 将预测值逆变换回原始尺度\nval_preds_v2 = np.expm1(val_preds_log_v2)\n\n# 计算并打印新的评估分数\nrmsle_v2 = np.sqrt(mean_squared_log_error(y_val, val_preds_v2))\n\nprint(\"-\" * 50)\nprint(f\"我们之前的基准分数为: 0.14561\")\nprint(f\"🎉 使用高级特征工程后，新分数为: {rmsle_v2:.5f}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:26:20.110711Z","iopub.execute_input":"2025-07-20T13:26:20.111032Z","iopub.status.idle":"2025-07-20T13:26:21.392663Z","shell.execute_reply.started":"2025-07-20T13:26:20.111009Z","shell.execute_reply":"2025-07-20T13:26:21.391890Z"}},"outputs":[{"name":"stdout","text":"正在使用加强版特征数据训练新模型...\n✅ 新模型训练完成！\n--------------------------------------------------\n我们之前的基准分数为: 0.14561\n🎉 使用高级特征工程后，新分数为: 0.14322\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"5.1.b.1 使用lightGBM","metadata":{}},{"cell_type":"code","source":"# LightGBM 是一个需要单独安装的库\n# 在Kaggle环境中，通常已经预装好了\n# 如果在本地，你可能需要运行 !pip install lightgbm\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_log_error\nimport numpy as np\n\n# --- 使用相同的“加强版”数据，但更换模型 ---\n\n# 1. 定义 LightGBM 回归器模型\n# 同样设置 random_state 以保证结果可复现\nmodel_lgbm = lgb.LGBMRegressor(random_state=42)\n\nprint(\"正在使用加强版特征数据训练 LightGBM 模型...\")\n# ‼️注意：我们用的仍然是 X_train_processed 和 y_train_log\nmodel_lgbm.fit(X_train_processed, y_train_log)\nprint(\"✅ LightGBM 模型训练完成！\")\n\n# 2. 在验证集上进行预测\nval_preds_log_lgbm = model_lgbm.predict(X_val_processed)\n\n# 3. 将预测值逆变换回原始尺度\nval_preds_lgbm = np.expm1(val_preds_log_lgbm)\n\n# 4. 计算并打印新的评估分数\nrmsle_lgbm = np.sqrt(mean_squared_log_error(y_val, val_preds_lgbm))\n\nprint(\"-\" * 50)\nprint(f\"随机森林模型的分数是: 0.14544\")\nprint(f\"🚀 使用 LightGBM 模型后，新分数为: {rmsle_lgbm:.5f}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:26:34.270068Z","iopub.execute_input":"2025-07-20T13:26:34.270376Z","iopub.status.idle":"2025-07-20T13:26:34.461219Z","shell.execute_reply.started":"2025-07-20T13:26:34.270355Z","shell.execute_reply":"2025-07-20T13:26:34.460291Z"}},"outputs":[{"name":"stdout","text":"正在使用加强版特征数据训练 LightGBM 模型...\n✅ LightGBM 模型训练完成！\n--------------------------------------------------\n随机森林模型的分数是: 0.14544\n🚀 使用 LightGBM 模型后，新分数为: 0.13299\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"5.1.b.2 lightGBM超参数调优","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.metrics import mean_squared_log_error\n\n# --- 使用我们之前处理好的、最强的数据 ---\n# 确保 X_train_processed, y_train_log, X_val_processed, y_val 都已存在\n\nprint(\"开始进行LightGBM的超参数调优...\")\n\n# 1. 定义我们要搜索的参数网格\n# 我们只选几个关键参数进行一个初步的、较快的搜索\nparam_grid = {\n    'n_estimators': [100, 200, 500],\n    'learning_rate': [0.05, 0.1],\n    'num_leaves': [20, 31, 40],\n    'random_state': [42] # 固定随机种子以复现结果\n}\n\n# 2. 初始化LGBM模型和GridSearchCV\nlgbm = lgb.LGBMRegressor(verbosity=-1)\ngrid_search = GridSearchCV(estimator=lgbm, \n                           param_grid=param_grid, \n                           cv=3, # 3折交叉验证以节省时间\n                           scoring='neg_root_mean_squared_error', # 使用负的RMSE作为评分标准\n                           n_jobs=-1, # 使用所有CPU核心\n                           verbose=1) # 打印搜索过程\n\n# 3. 在【对数变换后】的训练数据上进行搜索\n# GridSearchCV会自动处理交叉验证，所以我们用全部的训练数据\ngrid_search.fit(X_train_processed, y_train_log)\n\nprint(\"\\n✅ 超参数调优完成！\")\nprint(\"-\" * 50)\nprint(f\"找到的最佳参数组合是: {grid_search.best_params_}\")\nprint(f\"在交叉验证中得到的最佳分数为: {-grid_search.best_score_:.5f}\") # 分数要取反\nprint(\"-\" * 50)\n\n\n# 4. 使用找到的最佳模型在验证集上进行最终评估\nbest_lgbm_model = grid_search.best_estimator_\nval_preds_log_best = best_lgbm_model.predict(X_val_processed)\nval_preds_best_lgbm= np.expm1(val_preds_log_best)\n\nrmsle_best = np.sqrt(mean_squared_log_error(y_val, val_preds_best_lgbm))\n\nprint(f\"我们之前的单模型分数是: 0.13702\")\nprint(f\"⚙️ 经过精细调优后，新分数为: {rmsle_best:.5f}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:26:37.556727Z","iopub.execute_input":"2025-07-20T13:26:37.557503Z","iopub.status.idle":"2025-07-20T13:27:24.092411Z","shell.execute_reply.started":"2025-07-20T13:26:37.557477Z","shell.execute_reply":"2025-07-20T13:27:24.091577Z"}},"outputs":[{"name":"stdout","text":"开始进行LightGBM的超参数调优...\nFitting 3 folds for each of 18 candidates, totalling 54 fits\n\n✅ 超参数调优完成！\n--------------------------------------------------\n找到的最佳参数组合是: {'learning_rate': 0.05, 'n_estimators': 200, 'num_leaves': 20, 'random_state': 42}\n在交叉验证中得到的最佳分数为: 0.12836\n--------------------------------------------------\n我们之前的单模型分数是: 0.13702\n⚙️ 经过精细调优后，新分数为: 0.12969\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"5.1.c.1 使用XGBoost","metadata":{}},{"cell_type":"code","source":"# XGBoost 也是一个需要单独安装的库\n# 在Kaggle环境中，通常已经预装好了\n# 如果在本地，你可能需要运行 !pip install xgboost\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_log_error\nimport numpy as np\n\n# --- 使用相同的“加强版”数据，但更换为XGBoost模型 ---\n\n# 1. 定义 XGBoost 回归器模型\n# 我们给它更多的树(n_estimators=1000)，但用更小的学习率(learning_rate=0.05)\n# 同时设置了 early_stopping_rounds，让它在验证集上性能不再提升时自动停止训练\nmodel_xgb = xgb.XGBRegressor(n_estimators=1000, \n                             learning_rate=0.05, \n                             random_state=42, \n                             n_jobs=-1,\n                             early_stopping_rounds=5) # 核心：如果验证集分数连续5轮没有提升，就停止训练\n\nprint(\"正在使用加强版特征数据训练 XGBoost 模型...\")\n\n# 2. 训练模型，并加入提前停止的设置\n# ‼️注意：为了使用early_stopping_rounds，我们需要在.fit()中提供验证集数据\nmodel_xgb.fit(X_train_processed, y_train_log, \n              eval_set=[(X_val_processed, y_val_log)], \n              verbose=False) # verbose=False让它在训练中不打印过多信息\n\nprint(\"✅ XGBoost 模型训练完成！\")\n\n# 3. 在验证集上进行预测\nval_preds_log_xgb = model_xgb.predict(X_val_processed)\n\n# 4. 将预测值逆变换回原始尺度\nval_preds_xgb = np.expm1(val_preds_log_xgb)\n\n# 5. 计算并打印新的评估分数\n# 这里需要处理一个边界情况：如果模型预测出负数（虽然罕见），会导致计算错误\n# 我们确保所有预测值至少为0\nval_preds_xgb[val_preds_xgb < 0] = 0\nrmsle_xgb = np.sqrt(mean_squared_log_error(y_val, val_preds_xgb))\n\nprint(\"-\" * 50)\nprint(f\"LightGBM 模型的分数是: 0.13702\")\nprint(f\"🚀 使用 XGBoost 模型后，新分数为: {rmsle_xgb:.5f}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:29:07.096112Z","iopub.execute_input":"2025-07-20T13:29:07.096445Z","iopub.status.idle":"2025-07-20T13:29:07.933433Z","shell.execute_reply.started":"2025-07-20T13:29:07.096422Z","shell.execute_reply":"2025-07-20T13:29:07.932781Z"}},"outputs":[{"name":"stdout","text":"正在使用加强版特征数据训练 XGBoost 模型...\n✅ XGBoost 模型训练完成！\n--------------------------------------------------\nLightGBM 模型的分数是: 0.13702\n🚀 使用 XGBoost 模型后，新分数为: 0.13171\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"5.1.c.2 XGB调优","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\nimport numpy as np\nfrom sklearn.metrics import mean_squared_log_error\n\nprint(\"开始进行XGBoost的超参数调优...\")\n\n# 1. 为XGBoost定义参数网格\n# 我们同样只选几个关键参数\nxgb_param_grid = {\n    'n_estimators': [200, 500],\n    'learning_rate': [0.05, 0.1],\n    'max_depth': [3, 5],\n    'random_state': [42]\n}\n\n# 2. 初始化XGBoost模型和GridSearchCV\nxgbr = xgb.XGBRegressor()\nxgb_grid_search = GridSearchCV(estimator=xgbr, \n                               param_grid=xgb_param_grid, \n                               cv=3, \n                               scoring='neg_root_mean_squared_error',\n                               n_jobs=-1,\n                               verbose=1)\n\n# 3. 在训练数据上进行搜索\nxgb_grid_search.fit(X_train_processed, y_train_log)\n\nprint(\"\\n✅ XGBoost超参数调优完成！\")\nprint(\"-\" * 50)\nprint(f\"找到的最佳参数组合是: {xgb_grid_search.best_params_}\")\nprint(f\"在交叉验证中得到的最佳分数为: {-xgb_grid_search.best_score_:.5f}\")\nprint(\"-\" * 50)\n\n# 4. 使用找到的最佳XGBoost模型在验证集上进行评估\nbest_xgb_model = xgb_grid_search.best_estimator_\nval_preds_log_best_xgb = best_xgb_model.predict(X_val_processed)\nval_preds_best_xgb = np.expm1(val_preds_log_best_xgb)\nval_preds_best_xgb[val_preds_best_xgb < 0] = 0 # 保证非负\n\nrmsle_best_xgb = np.sqrt(mean_squared_log_error(y_val, val_preds_best_xgb))\n\nprint(f\"我们之前的XGBoost单模型分数是: 0.13893\")\nprint(f\"⚙️ 经过精细调优后，XGBoost的新分数为: {rmsle_best_xgb:.5f}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:29:11.771633Z","iopub.execute_input":"2025-07-20T13:29:11.771951Z","iopub.status.idle":"2025-07-20T13:29:20.241095Z","shell.execute_reply.started":"2025-07-20T13:29:11.771929Z","shell.execute_reply":"2025-07-20T13:29:20.240406Z"}},"outputs":[{"name":"stdout","text":"开始进行XGBoost的超参数调优...\nFitting 3 folds for each of 8 candidates, totalling 24 fits\n\n✅ XGBoost超参数调优完成！\n--------------------------------------------------\n找到的最佳参数组合是: {'learning_rate': 0.05, 'max_depth': 3, 'n_estimators': 500, 'random_state': 42}\n在交叉验证中得到的最佳分数为: 0.12107\n--------------------------------------------------\n我们之前的XGBoost单模型分数是: 0.13893\n⚙️ 经过精细调优后，XGBoost的新分数为: 0.12242\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"5.1.d catboost调优","metadata":{}},{"cell_type":"code","source":"# CatBoost 也是一个需要单独安装的库\n# 在Kaggle环境中，通常已经预装好了\n# 如果在本地，你可能需要运行 !pip install catboost\nimport catboost as cb\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nfrom sklearn.metrics import mean_squared_log_error\n\n# --- 我们使用特征工程后、独热编码前的数据 ---\n# 确保 X_train_pro, y_train_log, X_val_pro, y_val 都已存在\n\nprint(\"开始进行CatBoost的超参数调优...\")\n\n# 1. 首先，我们需要明确告诉CatBoost哪些是类别特征\n#    我们直接从 X_train_pro 数据框中获取类别列的列名\ncategorical_cols_names = [cname for cname in X_train_pro.columns if X_train_pro[cname].dtype == \"object\"]\n\n\n# 2. 定义我们要搜索的参数网格 (不变)\ncat_param_grid = {\n    'iterations': [200, 500],\n    'learning_rate': [0.05, 0.1],\n    'depth': [4, 6],\n    'random_state': [42]\n}\n\n# 3. 初始化CatBoost模型和GridSearchCV (不变)\nmodel_cat = cb.CatBoostRegressor(cat_features=categorical_cols_names, verbose=0)\ncat_grid_search = GridSearchCV(estimator=model_cat, \n                               param_grid=cat_param_grid, \n                               cv=3, \n                               scoring='neg_root_mean_squared_error',\n                               n_jobs=-1)\n\n# 4. 在【未经独热编码】的训练数据上进行搜索 (正确)\ncat_grid_search.fit(X_train_pro, y_train_log)\n\nprint(\"\\n✅ CatBoost超参数调优完成！\")\nprint(\"-\" * 50)\nprint(f\"找到的最佳参数组合是: {cat_grid_search.best_params_}\")\nprint(f\"在交叉验证中得到的最佳分数为: {-cat_grid_search.best_score_:.5f}\")\nprint(\"-\" * 50)\n\n\n# 5. 使用找到的最佳CatBoost模型在验证集上进行最终评估\nbest_cat_model = cat_grid_search.best_estimator_\n\n# ‼️【修正】: 预测时也必须使用未经独热编码的 X_val_pro\nval_preds_log_best_cat = best_cat_model.predict(X_val_pro)\nval_preds_best_cat = np.expm1(val_preds_log_best_cat)\n\nrmsle_best_cat = np.sqrt(mean_squared_log_error(y_val, val_preds_best_cat))\n\n# 更新一下对比基准为我们当前最好的单模型分数\nprint(f\"我们之前的最好成绩(调优后XGBoost)是: 0.12242\")\nprint(f\"🚀 使用调优后的 CatBoost 模型，新分数为: {rmsle_best_cat:.5f}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:29:33.109154Z","iopub.execute_input":"2025-07-20T13:29:33.109773Z","iopub.status.idle":"2025-07-20T13:31:32.444134Z","shell.execute_reply.started":"2025-07-20T13:29:33.109748Z","shell.execute_reply":"2025-07-20T13:31:32.443209Z"}},"outputs":[{"name":"stdout","text":"开始进行CatBoost的超参数调优...\n\n✅ CatBoost超参数调优完成！\n--------------------------------------------------\n找到的最佳参数组合是: {'depth': 6, 'iterations': 500, 'learning_rate': 0.1, 'random_state': 42}\n在交叉验证中得到的最佳分数为: 0.12230\n--------------------------------------------------\n我们之前的最好成绩(调优后XGBoost)是: 0.12242\n🚀 使用调优后的 CatBoost 模型，新分数为: 0.12171\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"5.2 模型融合","metadata":{}},{"cell_type":"code","source":"# --- 假设以下变量来自你刚刚运行的、使用终极特征的几次模型训练 ---\n\n# val_preds_best_cat (来自终极数据 + 调优后CatBoost, 分数 0.12171)\n# val_preds_best_xgb (来自终极数据 + 调优后XGBoost, 分数 0.12242)\n# val_preds_best_lgbm (来自终极数据 + 调优后LightGBM, 分数 0.12969)\n\n# --- 让我们进行最终的加权融合 ---\n# 我们给新科冠军CatBoost最高的权重\nultimate_blend_preds = 0.6 * val_preds_best_cat + 0.2 * val_preds_best_xgb + 0.2 * val_preds_best_lgbm\nrmsle_ultimate_blend = np.sqrt(mean_squared_log_error(y_val, ultimate_blend_preds))\n\nprint(\"-\" * 50)\nprint(f\"调优后 CatBoost 单模型分数: 0.12171 (目前最好)\")\nprint(f\"调优后 XGBoost 单模型分数:  0.12242\")\nprint(f\"调优后 LightGBM 单模型分数: 0.12969\")\nprint(f\"👑👑👑 【终极融合】后，我们的毕业分数为: {rmsle_ultimate_blend:.5f}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:32:09.534521Z","iopub.execute_input":"2025-07-20T13:32:09.535148Z","iopub.status.idle":"2025-07-20T13:32:09.542140Z","shell.execute_reply.started":"2025-07-20T13:32:09.535124Z","shell.execute_reply":"2025-07-20T13:32:09.541222Z"}},"outputs":[{"name":"stdout","text":"--------------------------------------------------\n调优后 CatBoost 单模型分数: 0.12171 (目前最好)\n调优后 XGBoost 单模型分数:  0.12242\n调优后 LightGBM 单模型分数: 0.12969\n👑👑👑 【终极融合】后，我们的毕业分数为: 0.12005\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"6.最后生成预测结果","metadata":{}},{"cell_type":"code","source":"# 假设 preprocessor 和 model 变量已经存在于之前的代码块中\n\n# 1. 加载官方测试数据\ntest_data_path = '/kaggle/input/house-prices-advanced-regression-techniques/test.csv'\ntest_df = pd.read_csv(test_data_path)\n\n# 2. 准备测试数据 (保存Id，并去除Id列)\ntest_ids = test_df['Id']\ntest_X = test_df.drop('Id', axis=1)\n\n# 3. 使用【已经拟合过】的预处理器来转换测试数据\n# ‼️注意：这里只用 .transform()，绝不能用 .fit_transform()\ntest_X_processed = preprocessor.transform(test_X)\n\n# 4. 使用【已经训练好】的模型进行预测\ntest_preds_log = model.predict(test_X_processed)\n\n# 5. 将预测结果逆变换回原始尺度\ntest_preds = np.expm1(test_preds_log)\n\n# 6. 创建提交文件\nsubmission_df = pd.DataFrame({'Id': test_ids, 'SalePrice': test_preds})\n\n# 7. 保存为 .csv 文件\nsubmission_df.to_csv('submission_baseline.csv', index=False)\n\nprint(\"-\" * 50)\nprint(\"🎉 提交文件 'submission_baseline.csv' 已成功生成！\")\nprint(\"现在你可以去Kaggle的 'Output' 部分找到它并提交了。\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:32:12.558282Z","iopub.execute_input":"2025-07-20T13:32:12.558583Z","iopub.status.idle":"2025-07-20T13:32:12.609723Z","shell.execute_reply.started":"2025-07-20T13:32:12.558562Z","shell.execute_reply":"2025-07-20T13:32:12.608378Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3968590976.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 3. 使用【已经拟合过】的预处理器来转换测试数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# ‼️注意：这里只用 .transform()，绝不能用 .fit_transform()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtest_X_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 4. 使用【已经训练好】的模型进行预测\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         Xs = self._fit_transform(\n\u001b[0m\u001b[1;32m    801\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_fit_transform\u001b[0;34m(self, X, y, func, fitted, column_as_strings)\u001b[0m\n\u001b[1;32m    656\u001b[0m         )\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             return Parallel(n_jobs=self.n_jobs)(\n\u001b[0m\u001b[1;32m    659\u001b[0m                 delayed(func)(\n\u001b[1;32m    660\u001b[0m                     \u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1984\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1985\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1988\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m             \u001b[0;31m# Sequentially call the tasks and yield the results.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# pre_dispatch and n_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         iterable_with_config = (\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_with_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    659\u001b[0m                 delayed(func)(\n\u001b[1;32m    660\u001b[0m                     \u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m                     \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m                     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                     \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_pandas_indexing\u001b[0;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# check whether we should index with loc or iloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"int\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multi_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1018\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1020\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1021\u001b[0m             \u001b[0;31m# We should never have retval.ndim < self.ndim, as that should\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0;31m#  be handled by the _getitem_lowerdim call above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1418\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m         return self.obj._reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0maxis_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1558\u001b[0;31m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6198\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6251\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6252\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6254\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"['TotalSF', 'HouseAge', 'RemodAge', 'TotalBath', 'Qual_x_TotalSF', 'Qual_x_HouseAge'] not in index\""],"ename":"KeyError","evalue":"\"['TotalSF', 'HouseAge', 'RemodAge', 'TotalBath', 'Qual_x_TotalSF', 'Qual_x_HouseAge'] not in index\"","output_type":"error"}],"execution_count":45},{"cell_type":"code","source":"# ## 6. 生成最终提交文件 (独立模块版)\n\nimport pandas as pd\nimport numpy as np\n\nprint(\"开始生成最终提交文件...\")\n\n# --- 1. 加载官方测试数据 ---\ntest_df = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntest_ids = test_df['Id'] # 保存Id用于最终文件\n\n# --- 2. 应用我们最强的特征工程函数 ---\n# 核心：我们使用从训练集(X_train)中学到的skew_list_pro，应用到测试集上\nX_test_final, _ = feature_engineer_ultimate_final(test_df, skew_list_to_apply=skew_list_pro)\n\n# 从特征中去掉Id列，因为它不用于预测\nif 'Id' in X_test_final.columns:\n    X_test_final = X_test_final.drop('Id', axis=1)\n\n\n# --- 3. 分别为不同模型准备数据并进行预测 ---\n\n# a) CatBoost的预测 (在独热编码前的数据上)\nprint(\"CatBoost正在预测...\")\npreds_log_cat = best_cat_model.predict(X_test_final)\npreds_cat = np.expm1(preds_log_cat)\n\n\n# b) LGBM 和 XGBoost 的预测 (在独热编码后的数据上)\n# 使用我们之前在训练集上拟合好的preprocessor来转换测试数据\nprint(\"预处理器正在转换测试数据...\")\nX_test_processed = preprocessor.transform(X_test_final)\n\nprint(\"LightGBM正在预测...\")\npreds_log_lgbm = best_lgbm_model.predict(X_test_processed)\npreds_lgbm = np.expm1(preds_log_lgbm)\n\nprint(\"XGBoost正在预测...\")\npreds_log_xgb = best_xgb_model.predict(X_test_processed)\npreds_xgb = np.expm1(preds_log_xgb)\n\n\n# --- 4. 终极加权融合 ---\nprint(\"正在融合三个模型的预测结果...\")\n# 使用我们验证过的最佳权重\nfinal_predictions = 0.6 * preds_cat + 0.2 * preds_xgb + 0.2 * preds_lgbm\n\n\n# --- 5. 创建并保存提交文件 ---\nsubmission = pd.DataFrame({'Id': test_ids, 'SalePrice': final_predictions})\nsubmission.to_csv('submission_final_blend.csv', index=False)\n\nprint(\"-\" * 50)\nprint(\"🎉🎉🎉 最终提交文件 'submission_final_blend.csv' 已成功生成！🎉🎉🎉\")\nprint(\"你可以去 'Output' 部分找到它并提交到Kaggle。\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:32:20.216812Z","iopub.execute_input":"2025-07-20T13:32:20.217120Z","iopub.status.idle":"2025-07-20T13:32:20.374394Z","shell.execute_reply.started":"2025-07-20T13:32:20.217099Z","shell.execute_reply":"2025-07-20T13:32:20.373589Z"}},"outputs":[{"name":"stdout","text":"开始生成最终提交文件...\nCatBoost正在预测...\n预处理器正在转换测试数据...\nLightGBM正在预测...\nXGBoost正在预测...\n正在融合三个模型的预测结果...\n--------------------------------------------------\n🎉🎉🎉 最终提交文件 'submission_final_blend.csv' 已成功生成！🎉🎉🎉\n你可以去 'Output' 部分找到它并提交到Kaggle。\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# 假设 X_train_pro 和 X_val_pro (或你最终命名的变量) 已经存在\n# 我们用 X_train_pro 和 X_test_final (来自你上个代码块) 来比较\n\nprint(\"--- 开始排查训练集和测试集的类别差异 ---\")\n\n# 找出两个数据集中共有的类别列\ncommon_categorical_cols = [col for col in categorical_cols if col in X_test_final.columns]\n\n# 逐一比较每个类别列\nfor col in common_categorical_cols:\n    train_categories = set(X_train_pro[col].unique())\n    test_categories = set(X_test_final[col].unique())\n\n    # 找出只在测试集中出现的新类别\n    new_categories_in_test = test_categories - train_categories\n\n    if new_categories_in_test:\n        print(f\"\\n‼️ 在特征 '{col}' 中发现问题！\")\n        print(f\"   -> 测试集出现了训练集中没有的新类别: {new_categories_in_test}\")\n\nprint(\"\\n--- 排查完毕 ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-20T13:22:35.883423Z","iopub.execute_input":"2025-07-20T13:22:35.884019Z","iopub.status.idle":"2025-07-20T13:22:35.898634Z","shell.execute_reply.started":"2025-07-20T13:22:35.883992Z","shell.execute_reply":"2025-07-20T13:22:35.897910Z"}},"outputs":[{"name":"stdout","text":"--- 开始排查训练集和测试集的类别差异 ---\n\n‼️ 在特征 'MSSubClass' 中发现问题！\n   -> 测试集出现了训练集中没有的新类别: {'150'}\n\n‼️ 在特征 'Functional' 中发现问题！\n   -> 测试集出现了训练集中没有的新类别: {'Sev'}\n\n--- 排查完毕 ---\n","output_type":"stream"}],"execution_count":36}]}