{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1.数据加载与切分","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# --- 我们先重新加载和切分数据，确保所有步骤都在一个代码块里 ---\n# ## 1. 数据加载与切分\n\n# 加载数据\nfile_path = '/kaggle/input/house-prices-advanced-regression-techniques/train.csv'\ntrain_full = pd.read_csv(file_path)\n\n# 分离特征 (X) 和目标 (y)\nX_full = train_full.drop('SalePrice', axis=1)\ny_full = train_full['SalePrice']\n\n# --- **就放在这里！** ---\n# 在切分之前，就从总的特征集里把Id去掉\nX_full_no_id = X_full.drop('Id', axis=1)\n# -------------------------\n\n# 然后，用不含Id的特征集去进行切分\nX_train, X_val, y_train, y_val = train_test_split(X_full_no_id, y_full, test_size=0.2, random_state=42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-19T09:18:02.884175Z","iopub.execute_input":"2025-07-19T09:18:02.885343Z","iopub.status.idle":"2025-07-19T09:18:02.927196Z","shell.execute_reply.started":"2025-07-19T09:18:02.885306Z","shell.execute_reply":"2025-07-19T09:18:02.926053Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"2.对数变换","metadata":{}},{"cell_type":"code","source":"\n# 对目标变量y进行对数变换\ny_train_log = np.log1p(y_train)\ny_val_log = np.log1p(y_val)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T09:18:05.331651Z","iopub.execute_input":"2025-07-19T09:18:05.331983Z","iopub.status.idle":"2025-07-19T09:18:05.337615Z","shell.execute_reply.started":"2025-07-19T09:18:05.331958Z","shell.execute_reply":"2025-07-19T09:18:05.336502Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"3.特征工程","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\n# 这是一个独立的函数，我们可以反复调用它\ndef feature_engineer_advanced(df):\n    \"\"\"\n    对数据集进行高级特征工程，包括智能填充和特征创造。\n    \"\"\"\n    # 为了不修改原始数据，先创建副本\n    df_fe = df.copy()\n\n    # --- 1. 智能填充：NaN 代表 \"没有\" ---\n    \n    # 填充类别型特征\n    for col in ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n                'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n                'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n                'MasVnrType']:\n        df_fe[col] = df_fe[col].fillna('None')\n\n    # 填充数值型特征\n    for col in ['GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2',\n                'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath',\n                'MasVnrArea']:\n        df_fe[col] = df_fe[col].fillna(0)\n\n    # --- 2. 智能填充：用分组中位数填充 \"真缺失\" ---\n\n    # LotFrontage 很可能和街区(Neighborhood)有关\n    # 我们用每个街区LotFrontage的中位数来填充该街区的缺失值\n    df_fe['LotFrontage'] = df_fe.groupby('Neighborhood')['LotFrontage'].transform(\n        lambda x: x.fillna(x.median())\n    )\n\n    # 对于其他一些\"真缺失\"，我们暂时还用众数填充，作为保底策略\n    for col in ['MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType']:\n        df_fe[col] = df_fe[col].fillna(df_fe[col].mode()[0])\n\n\n    # --- 3. 创造新特征 (和我们上次做的一样) ---\n\n    df_fe['TotalSF'] = df_fe['TotalBsmtSF'] + df_fe['1stFlrSF'] + df_fe['2ndFlrSF']\n    df_fe['HouseAge'] = df_fe['YrSold'] - df_fe['YearBuilt']\n    df_fe['RemodAge'] = df_fe['YrSold'] - df_fe['YearRemodAdd']\n    df_fe['TotalBath'] = df_fe['BsmtFullBath'] + (0.5 * df_fe['BsmtHalfBath']) + \\\n                         df_fe['FullBath'] + (0.5 * df_fe['HalfBath'])\n    \n    # --- 4. (可选) 删除一些不再需要的原始列 ---\n    # 比如我们已经有了HouseAge，YearBuilt可能就不那么重要了\n    # df_fe = df_fe.drop(['YearBuilt', 'YearRemodAdd'], axis=1)\n\n    print(\"✅ 高级特征工程函数已定义。\")\n    return df_fe\n\n# --- 现在，我们用这个新函数来处理我们的训练集和验证集 ---\n# 假设 X_train 和 X_val 已经存在\nX_train_advanced = feature_engineer_advanced(X_train)\nX_val_advanced = feature_engineer_advanced(X_val)\n\nprint(\"\\n查看一下处理后的数据，确认缺失值是否已被处理：\")\n# 检查处理后的训练集是否还有缺失值\nmissing_after = X_train_advanced.isnull().sum().sum()\nprint(f\"处理后，训练集剩余缺失值总数: {missing_after}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T09:18:07.843956Z","iopub.execute_input":"2025-07-19T09:18:07.844334Z","iopub.status.idle":"2025-07-19T09:18:08.018455Z","shell.execute_reply.started":"2025-07-19T09:18:07.844308Z","shell.execute_reply":"2025-07-19T09:18:08.017080Z"}},"outputs":[{"name":"stdout","text":"✅ 高级特征工程函数已定义。\n✅ 高级特征工程函数已定义。\n\n查看一下处理后的数据，确认缺失值是否已被处理：\n处理后，训练集剩余缺失值总数: 0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"4.预处理pipeline","metadata":{}},{"cell_type":"code","source":"# --- ## 4. 数据预处理流水线 (Preprocessing Pipeline) ---\n\n# 首先，我们从经过特征工程的数据中舍去Id列\n# （虽然我们之前的函数没用到Id，但在这里再确认一遍是好习惯）\nif 'Id' in X_train_advanced.columns:\n    X_train_final = X_train_advanced.drop('Id', axis=1)\n    X_val_final = X_val_advanced.drop('Id', axis=1)\nelse:\n    X_train_final = X_train_advanced\n    X_val_final = X_val_advanced\n\n# 1. 重新定义数值列和类别列的列表\n# 因为我们增加了新特征（如TotalSF），所以需要更新列表\ncategorical_cols = [cname for cname in X_train_final.columns if X_train_final[cname].dtype == \"object\"]\nnumerical_cols = [cname for cname in X_train_final.columns if X_train_final[cname].dtype in ['int64', 'float64']]\n\n# 2. 创建预处理管道 (和之前一样，但现在作用于新数据)\n# 对于数值型特征，我们只保留一个中位数填充器，以防万一有未处理的缺失值\nnumerical_transformer = SimpleImputer(strategy='median')\n\n# 对于类别型特征，我们只需要进行独热编码\n# 因为我们的特征工程函数已经填充了所有类别缺失值\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# 3. 组合成一个总的预处理器\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n\n# 4. 用预处理器来处理经过高级特征工程的数据\nX_train_processed = preprocessor.fit_transform(X_train_final)\nX_val_processed = preprocessor.transform(X_val_final)\n\n\n# 5. 查看最终处理后的数据形状\nprint(\"高级特征工程后，训练集形状:\", X_train_final.shape)\nprint(\"最终处理后，训练集形状:\", X_train_processed.shape)\nprint(\"\\n高级特征工程后，验证集形状:\", X_val_final.shape)\nprint(\"最终处理后，验证集形状:\", X_val_processed.shape)\n\nprint(\"\\n✅ 所有数据准备工作已完成！现在可以进入模型训练了。\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T09:19:56.882797Z","iopub.execute_input":"2025-07-19T09:19:56.883172Z","iopub.status.idle":"2025-07-19T09:19:57.044289Z","shell.execute_reply.started":"2025-07-19T09:19:56.883149Z","shell.execute_reply":"2025-07-19T09:19:57.042940Z"}},"outputs":[{"name":"stdout","text":"高级特征工程后，训练集形状: (1168, 83)\n最终处理后，训练集形状: (1168, 304)\n\n高级特征工程后，验证集形状: (292, 83)\n最终处理后，验证集形状: (292, 304)\n\n✅ 所有数据准备工作已完成！现在可以进入模型训练了。\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"5.1.a 使用random forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_log_error\nimport numpy as np\n\n# --- 再次训练模型，但这次用的是经过高级特征工程的数据 ---\n\n# 我们可以叫它 model_v2，以区别于之前的基准模型\nmodel_v2 = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n\nprint(\"正在使用加强版特征数据训练新模型...\")\n# ‼️注意：我们用的是 X_train_processed 和 y_train_log\nmodel_v2.fit(X_train_processed, y_train_log)\nprint(\"✅ 新模型训练完成！\")\n\n# 在验证集上进行预测\nval_preds_log_v2 = model_v2.predict(X_val_processed)\n\n# 将预测值逆变换回原始尺度\nval_preds_v2 = np.expm1(val_preds_log_v2)\n\n# 计算并打印新的评估分数\nrmsle_v2 = np.sqrt(mean_squared_log_error(y_val, val_preds_v2))\n\nprint(\"-\" * 50)\nprint(f\"我们之前的基准分数为: 0.14561\")\nprint(f\"🎉 使用高级特征工程后，新分数为: {rmsle_v2:.5f}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T09:22:25.256955Z","iopub.execute_input":"2025-07-19T09:22:25.257962Z","iopub.status.idle":"2025-07-19T09:22:30.359936Z","shell.execute_reply.started":"2025-07-19T09:22:25.257928Z","shell.execute_reply":"2025-07-19T09:22:30.359039Z"}},"outputs":[{"name":"stdout","text":"正在使用加强版特征数据训练新模型...\n✅ 新模型训练完成！\n--------------------------------------------------\n我们之前的基准分数为: 0.14561\n🎉 使用高级特征工程后，新分数为: 0.14544\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"5.1.b 使用lightGBM","metadata":{}},{"cell_type":"code","source":"# LightGBM 是一个需要单独安装的库\n# 在Kaggle环境中，通常已经预装好了\n# 如果在本地，你可能需要运行 !pip install lightgbm\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_log_error\nimport numpy as np\n\n# --- 使用相同的“加强版”数据，但更换模型 ---\n\n# 1. 定义 LightGBM 回归器模型\n# 同样设置 random_state 以保证结果可复现\nmodel_lgbm = lgb.LGBMRegressor(random_state=42)\n\nprint(\"正在使用加强版特征数据训练 LightGBM 模型...\")\n# ‼️注意：我们用的仍然是 X_train_processed 和 y_train_log\nmodel_lgbm.fit(X_train_processed, y_train_log)\nprint(\"✅ LightGBM 模型训练完成！\")\n\n# 2. 在验证集上进行预测\nval_preds_log_lgbm = model_lgbm.predict(X_val_processed)\n\n# 3. 将预测值逆变换回原始尺度\nval_preds_lgbm = np.expm1(val_preds_log_lgbm)\n\n# 4. 计算并打印新的评估分数\nrmsle_lgbm = np.sqrt(mean_squared_log_error(y_val, val_preds_lgbm))\n\nprint(\"-\" * 50)\nprint(f\"随机森林模型的分数是: 0.14544\")\nprint(f\"🚀 使用 LightGBM 模型后，新分数为: {rmsle_lgbm:.5f}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T09:27:39.388107Z","iopub.execute_input":"2025-07-19T09:27:39.388444Z","iopub.status.idle":"2025-07-19T09:27:46.359111Z","shell.execute_reply.started":"2025-07-19T09:27:39.388420Z","shell.execute_reply":"2025-07-19T09:27:46.357929Z"}},"outputs":[{"name":"stdout","text":"正在使用加强版特征数据训练 LightGBM 模型...\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003336 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 3653\n[LightGBM] [Info] Number of data points in the train set: 1168, number of used features: 197\n[LightGBM] [Info] Start training from score 12.030658\n✅ LightGBM 模型训练完成！\n--------------------------------------------------\n随机森林模型的分数是: 0.14544\n🚀 使用 LightGBM 模型后，新分数为: 0.13702\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"5.1.c 使用XGBoost","metadata":{}},{"cell_type":"code","source":"# XGBoost 也是一个需要单独安装的库\n# 在Kaggle环境中，通常已经预装好了\n# 如果在本地，你可能需要运行 !pip install xgboost\nimport xgboost as xgb\nfrom sklearn.metrics import mean_squared_log_error\nimport numpy as np\n\n# --- 使用相同的“加强版”数据，但更换为XGBoost模型 ---\n\n# 1. 定义 XGBoost 回归器模型\n# 我们给它更多的树(n_estimators=1000)，但用更小的学习率(learning_rate=0.05)\n# 同时设置了 early_stopping_rounds，让它在验证集上性能不再提升时自动停止训练\nmodel_xgb = xgb.XGBRegressor(n_estimators=1000, \n                             learning_rate=0.05, \n                             random_state=42, \n                             n_jobs=-1,\n                             early_stopping_rounds=5) # 核心：如果验证集分数连续5轮没有提升，就停止训练\n\nprint(\"正在使用加强版特征数据训练 XGBoost 模型...\")\n\n# 2. 训练模型，并加入提前停止的设置\n# ‼️注意：为了使用early_stopping_rounds，我们需要在.fit()中提供验证集数据\nmodel_xgb.fit(X_train_processed, y_train_log, \n              eval_set=[(X_val_processed, y_val_log)], \n              verbose=False) # verbose=False让它在训练中不打印过多信息\n\nprint(\"✅ XGBoost 模型训练完成！\")\n\n# 3. 在验证集上进行预测\nval_preds_log_xgb = model_xgb.predict(X_val_processed)\n\n# 4. 将预测值逆变换回原始尺度\nval_preds_xgb = np.expm1(val_preds_log_xgb)\n\n# 5. 计算并打印新的评估分数\n# 这里需要处理一个边界情况：如果模型预测出负数（虽然罕见），会导致计算错误\n# 我们确保所有预测值至少为0\nval_preds_xgb[val_preds_xgb < 0] = 0\nrmsle_xgb = np.sqrt(mean_squared_log_error(y_val, val_preds_xgb))\n\nprint(\"-\" * 50)\nprint(f\"LightGBM 模型的分数是: 0.13702\")\nprint(f\"🚀 使用 XGBoost 模型后，新分数为: {rmsle_xgb:.5f}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T10:07:45.887188Z","iopub.execute_input":"2025-07-19T10:07:45.887528Z","iopub.status.idle":"2025-07-19T10:07:47.299489Z","shell.execute_reply.started":"2025-07-19T10:07:45.887503Z","shell.execute_reply":"2025-07-19T10:07:47.298547Z"}},"outputs":[{"name":"stdout","text":"正在使用加强版特征数据训练 XGBoost 模型...\n✅ XGBoost 模型训练完成！\n--------------------------------------------------\nLightGBM 模型的分数是: 0.13702\n🚀 使用 XGBoost 模型后，新分数为: 0.13893\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"5.2 模型融合","metadata":{}},{"cell_type":"code","source":"# 我们已经有了上一轮的预测结果：\n# val_preds_lgbm (LightGBM的预测)\n# val_preds_xgb (XGBoost的预测)\n\n# 1. 将两个模型的预测结果做简单的平均\nblended_preds = 0.5 * val_preds_lgbm + 0.5 * val_preds_xgb\n\n# 2. 计算融合后的分数\nrmsle_blended = np.sqrt(mean_squared_log_error(y_val, blended_preds))\n\nprint(\"-\" * 50)\nprint(f\"LightGBM 单模型分数: 0.13702\")\nprint(f\"XGBoost 单模型分数:  0.13893\")\nprint(f\"🤝 两个模型融合后，最终分数为: {rmsle_blended:.5f}\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T10:08:11.349291Z","iopub.execute_input":"2025-07-19T10:08:11.349708Z","iopub.status.idle":"2025-07-19T10:08:11.360179Z","shell.execute_reply.started":"2025-07-19T10:08:11.349677Z","shell.execute_reply":"2025-07-19T10:08:11.359063Z"}},"outputs":[{"name":"stdout","text":"--------------------------------------------------\nLightGBM 单模型分数: 0.13702\nXGBoost 单模型分数:  0.13893\n🤝 两个模型融合后，最终分数为: 0.13590\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"6.最后生成预测结果","metadata":{}},{"cell_type":"code","source":"# 假设 preprocessor 和 model 变量已经存在于之前的代码块中\n\n# 1. 加载官方测试数据\ntest_data_path = '/kaggle/input/house-prices-advanced-regression-techniques/test.csv'\ntest_df = pd.read_csv(test_data_path)\n\n# 2. 准备测试数据 (保存Id，并去除Id列)\ntest_ids = test_df['Id']\ntest_X = test_df.drop('Id', axis=1)\n\n# 3. 使用【已经拟合过】的预处理器来转换测试数据\n# ‼️注意：这里只用 .transform()，绝不能用 .fit_transform()\ntest_X_processed = preprocessor.transform(test_X)\n\n# 4. 使用【已经训练好】的模型进行预测\ntest_preds_log = model.predict(test_X_processed)\n\n# 5. 将预测结果逆变换回原始尺度\ntest_preds = np.expm1(test_preds_log)\n\n# 6. 创建提交文件\nsubmission_df = pd.DataFrame({'Id': test_ids, 'SalePrice': test_preds})\n\n# 7. 保存为 .csv 文件\nsubmission_df.to_csv('submission_baseline.csv', index=False)\n\nprint(\"-\" * 50)\nprint(\"🎉 提交文件 'submission_baseline.csv' 已成功生成！\")\nprint(\"现在你可以去Kaggle的 'Output' 部分找到它并提交了。\")\nprint(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T11:52:05.906218Z","iopub.execute_input":"2025-07-18T11:52:05.907082Z","iopub.status.idle":"2025-07-18T11:52:06.019457Z","shell.execute_reply.started":"2025-07-18T11:52:05.907053Z","shell.execute_reply":"2025-07-18T11:52:06.018659Z"}},"outputs":[{"name":"stdout","text":"--------------------------------------------------\n🎉 提交文件 'submission_baseline.csv' 已成功生成！\n现在你可以去Kaggle的 'Output' 部分找到它并提交了。\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":6}]}